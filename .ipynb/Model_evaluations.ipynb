{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/didulabhanuka/Tomato-Ripeness-Classifier/blob/main/Model_evaluations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lZfRw7cBTQ6w"
      },
      "outputs": [],
      "source": [
        "pip install ultralytics torch torchvision torchmetrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5hsvONvrThH7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import yaml\n",
        "import torch\n",
        "import random\n",
        "import shutil\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "import torchvision.transforms as transforms\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.transforms import functional as F\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score, precision_recall_fscore_support\n",
        "from albumentations import Compose, RandomBrightnessContrast, HueSaturationValue, GaussianBlur, MotionBlur, Normalize\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIGqZ7PuUwv9"
      },
      "outputs": [],
      "source": [
        "\n",
        "yaml_content = \"\"\"\n",
        "train: /content/drive/MyDrive/tomato_ripeness_classifier/dataset_augmented/train\n",
        "val: /content/drive/MyDrive/tomato_ripeness_classifier/dataset_augmented/val\n",
        "\n",
        "nc: 6\n",
        "names: [\"b_fully_ripened\", \"b_half_ripened\", \"b_green\", \"l_fully_ripened\", \"l_half_ripened\", \"l_green\"]\n",
        "\"\"\"\n",
        "\n",
        "yaml_path = \"/content/tomato_ripeness_classifier.yaml\"\n",
        "with open(yaml_path, \"w\") as file:\n",
        "    file.write(yaml_content)\n",
        "\n",
        "print(f\"YAML file created at {yaml_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeoJICuUVyic"
      },
      "outputs": [],
      "source": [
        "dataset_zip = \"/content/drive/MyDrive/tomato_ripeness_classifier/tomato_dataset.zip\"\n",
        "dataset_dir = \"/content/drive/MyDrive/tomato_ripeness_classifier/dataset/dataset\"\n",
        "\n",
        "os.makedirs(\"/content/dataset\", exist_ok=True)\n",
        "with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/dataset\")\n",
        "\n",
        "print(\"Dataset unzipped successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6hFJzDvIVlu5"
      },
      "outputs": [],
      "source": [
        "# Define dataset paths\n",
        "augmented_dir = \"/content/drive/MyDrive/tomato_ripeness_classifier/dataset_augmented\"\n",
        "\n",
        "# Create Augmented Dataset Folders\n",
        "os.makedirs(f\"{augmented_dir}/train/images\", exist_ok=True)\n",
        "os.makedirs(f\"{augmented_dir}/train/labels\", exist_ok=True)\n",
        "os.makedirs(f\"{augmented_dir}/val/images\", exist_ok=True)\n",
        "os.makedirs(f\"{augmented_dir}/val/labels\", exist_ok=True)\n",
        "\n",
        "def denormalize_image(image):\n",
        "    \"\"\"\n",
        "    Reverts normalization to bring pixel values back to [0,255].\n",
        "    \"\"\"\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "    # Undo normalization\n",
        "    image = image * std + mean  # Reverse normalization\n",
        "    image = np.clip(image * 255, 0, 255).astype(np.uint8)  # Convert to 0-255 range\n",
        "\n",
        "    return image\n",
        "\n",
        "def validate_and_clip_bbox(bbox, img_w, img_h):\n",
        "    \"\"\"\n",
        "    Ensures bounding box values stay within valid ranges.\n",
        "    \"\"\"\n",
        "    x_center, y_center, width, height = bbox\n",
        "    x_center /= img_w\n",
        "    y_center /= img_h\n",
        "    width /= img_w\n",
        "    height /= img_h\n",
        "\n",
        "    x_center = np.clip(x_center, 0.0, 1.0)\n",
        "    y_center = np.clip(y_center, 0.0, 1.0)\n",
        "    width = np.clip(width, 0.0, 1.0)\n",
        "    height = np.clip(height, 0.0, 1.0)\n",
        "\n",
        "    x_min = x_center - width / 2\n",
        "    y_min = y_center - height / 2\n",
        "    x_max = x_center + width / 2\n",
        "    y_max = y_center + height / 2\n",
        "\n",
        "    if 0.0 <= x_min <= 1.0 and 0.0 <= y_min <= 1.0 and 0.0 <= x_max <= 1.0 and 0.0 <= y_max <= 1.0 and width > 0 and height > 0:\n",
        "        return [x_center, y_center, width, height]\n",
        "    return None  # Invalid bbox\n",
        "\n",
        "def advanced_augmentations(image_folder, label_folder, output_image_folder, output_label_folder):\n",
        "    \"\"\"\n",
        "    Applies augmentations while keeping bounding boxes correctly aligned.\n",
        "    \"\"\"\n",
        "    augmentations = Compose(\n",
        "        [\n",
        "            RandomBrightnessContrast(p=0.2),\n",
        "            HueSaturationValue(p=0.2),\n",
        "            GaussianBlur(p=0.1),\n",
        "            MotionBlur(p=0.1),\n",
        "            Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Applied for training\n",
        "        ],\n",
        "        bbox_params={\"format\": \"yolo\", \"label_fields\": [\"class_labels\"]},\n",
        "    )\n",
        "\n",
        "    for image_file in os.listdir(image_folder):\n",
        "        img_path = os.path.join(image_folder, image_file)\n",
        "        label_path = os.path.join(label_folder, os.path.splitext(image_file)[0] + \".txt\")\n",
        "\n",
        "        # Read the image\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is None:\n",
        "            print(f\"Skipping {image_file}: Unable to read image.\")\n",
        "            continue\n",
        "\n",
        "        h, w, _ = image.shape\n",
        "        bboxes = []\n",
        "        class_labels = []\n",
        "\n",
        "        # Read the bounding boxes\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, \"r\") as f:\n",
        "                for line in f.readlines():\n",
        "                    cls, x_center, y_center, width, height = map(float, line.strip().split())\n",
        "                    valid_bbox = validate_and_clip_bbox([x_center * w, y_center * h, width * w, height * h], w, h)\n",
        "                    if valid_bbox:\n",
        "                        bboxes.append(valid_bbox)\n",
        "                        class_labels.append(int(cls))\n",
        "\n",
        "        if not bboxes:\n",
        "            print(f\"Skipping image {image_file} due to no valid bounding boxes.\")\n",
        "            continue\n",
        "\n",
        "        # Apply Augmentations\n",
        "        augmented = augmentations(image=image, bboxes=bboxes, class_labels=class_labels)\n",
        "        augmented_image = augmented[\"image\"]\n",
        "        augmented_bboxes = augmented[\"bboxes\"]\n",
        "        augmented_class_labels = augmented[\"class_labels\"]\n",
        "\n",
        "        # ðŸ”¹ Fix Black Image Issue: Convert Back to uint8 before saving\n",
        "        augmented_image = denormalize_image(augmented_image)\n",
        "\n",
        "        # Save Augmented Image\n",
        "        output_img_path = os.path.join(output_image_folder, image_file)\n",
        "        cv2.imwrite(output_img_path, augmented_image)\n",
        "\n",
        "        # Save Updated Labels\n",
        "        output_label_path = os.path.join(output_label_folder, os.path.splitext(image_file)[0] + \".txt\")\n",
        "        with open(output_label_path, \"w\") as f:\n",
        "            for bbox, cls in zip(augmented_bboxes, augmented_class_labels):\n",
        "                x_center, y_center, width, height = bbox\n",
        "                f.write(f\"{cls} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
        "\n",
        "        print(f\"Saved Augmented Image: {output_img_path}\")\n",
        "\n",
        "# Apply Augmentation to Train and Validation Sets\n",
        "dataset_dir = \"/content/dataset/dataset\"\n",
        "advanced_augmentations(\n",
        "    image_folder=f\"{dataset_dir}/train/images\",\n",
        "    label_folder=f\"{dataset_dir}/train/labels\",\n",
        "    output_image_folder=f\"{augmented_dir}/train/images\",\n",
        "    output_label_folder=f\"{augmented_dir}/train/labels\"\n",
        ")\n",
        "\n",
        "advanced_augmentations(\n",
        "    image_folder=f\"{dataset_dir}/val/images\",\n",
        "    label_folder=f\"{dataset_dir}/val/labels\",\n",
        "    output_image_folder=f\"{augmented_dir}/val/images\",\n",
        "    output_label_folder=f\"{augmented_dir}/val/labels\"\n",
        ")\n",
        "\n",
        "print(\"âœ… Augmentation completed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gximroeUtYX"
      },
      "outputs": [],
      "source": [
        "# Load the trained YOLOv8 model\n",
        "model = YOLO(\"/content/drive/MyDrive/tomato_ripeness_classifier/yolov8_tomato_ripeness/weights/best.pt\")\n",
        "\n",
        "# Run evaluation on the validation set\n",
        "metrics = model.val()\n",
        "\n",
        "# Extract key metrics\n",
        "precision = metrics.box.p  # Precision values per class\n",
        "recall = metrics.box.r      # Recall values per class\n",
        "map50 = metrics.box.map50  # mAP@50 values per class\n",
        "map50_95 = metrics.box.map  # mAP@50-95 values per class\n",
        "classes = metrics.names     # Class names\n",
        "\n",
        "\n",
        "# Convert metrics to lists\n",
        "class_labels = list(classes.values())  # Extract class names\n",
        "num_classes = len(class_labels)\n",
        "\n",
        "# Plot Precision & Recall\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(class_labels, precision, color=\"blue\", label=\"Precision\")\n",
        "plt.bar(class_labels, recall, color=\"red\", alpha=0.7, label=\"Recall\")\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Precision & Recall per Class\")\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.savefig(\"/content/precision_recall_plot.png\")\n",
        "plt.show()\n",
        "\n",
        "# Plot mAP@50 and mAP@50-95\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(class_labels, map50, color=\"green\", label=\"mAP@50\")\n",
        "plt.bar(class_labels, map50_95, color=\"orange\", alpha=0.7, label=\"mAP@50-95\")\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"mAP Scores per Class\")\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.savefig(\"/content/map_plot.png\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Evaluation Complete. Saved plots as images.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgDcrwjwFKQA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "\n",
        "# Paths to your dataset\n",
        "image_dir = \"/content/drive/MyDrive/tomato_ripeness_classifier/dataset_augmented/val/images\"\n",
        "label_dir = \"/content/drive/MyDrive/tomato_ripeness_classifier/dataset_augmented/val/labels\"\n",
        "output_json = \"/content/drive/MyDrive/tomato_ripeness_classifier/dataset_augmented/val/annotations.json\"\n",
        "\n",
        "# Load class names from YAML\n",
        "class_names = [\"b_fully_ripened\", \"b_half_ripened\", \"b_green\", \"l_fully_ripened\", \"l_half_ripened\", \"l_green\"]\n",
        "class_mapping = {i: name for i, name in enumerate(class_names)}\n",
        "\n",
        "# COCO JSON structure\n",
        "coco_data = {\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": [{\"id\": i, \"name\": name, \"supercategory\": \"none\"} for i, name in class_mapping.items()]\n",
        "}\n",
        "\n",
        "annotation_id = 0\n",
        "for img_id, img_name in enumerate(os.listdir(image_dir)):\n",
        "    if not img_name.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
        "        continue\n",
        "\n",
        "    # Image details\n",
        "    img_path = os.path.join(image_dir, img_name)\n",
        "    img = Image.open(img_path)\n",
        "    width, height = img.size\n",
        "\n",
        "    coco_data[\"images\"].append({\n",
        "        \"id\": img_id,\n",
        "        \"file_name\": img_name,\n",
        "        \"width\": width,\n",
        "        \"height\": height\n",
        "    })\n",
        "\n",
        "    # Corresponding YOLO label file\n",
        "    label_file = os.path.join(label_dir, os.path.splitext(img_name)[0] + \".txt\")\n",
        "    if not os.path.exists(label_file):\n",
        "        continue\n",
        "\n",
        "    with open(label_file, \"r\") as f:\n",
        "        for line in f.readlines():\n",
        "            parts = line.strip().split()\n",
        "            # Convert to int after converting to float to handle '3.0' like values\n",
        "            class_id = int(float(parts[0]))\n",
        "            x_center, y_center, bbox_width, bbox_height = map(float, parts[1:])\n",
        "\n",
        "            # Convert YOLO format (normalized) to COCO format (absolute pixel values)\n",
        "            x_min = (x_center - bbox_width / 2) * width\n",
        "            y_min = (y_center - bbox_height / 2) * height\n",
        "            bbox_width *= width\n",
        "            bbox_height *= height\n",
        "\n",
        "            coco_data[\"annotations\"].append({\n",
        "                \"id\": annotation_id,\n",
        "                \"image_id\": img_id,\n",
        "                \"category_id\": class_id,\n",
        "                \"bbox\": [x_min, y_min, bbox_width, bbox_height],\n",
        "                \"area\": bbox_width * bbox_height,\n",
        "                \"iscrowd\": 0\n",
        "            })\n",
        "            annotation_id += 1\n",
        "\n",
        "# Save to COCO JSON file\n",
        "with open(output_json, \"w\") as f:\n",
        "    json.dump(coco_data, f, indent=4)\n",
        "\n",
        "print(f\"COCO annotations saved to {output_json}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YswWXCjJCnG"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "image_dir = \"/content/drive/MyDrive/tomato_ripeness_classifier/dataset_augmented/val/images\"\n",
        "annotation_file = \"/content/drive/MyDrive/tomato_ripeness_classifier/dataset_augmented/val/annotations.json\"\n",
        "\n",
        "# Load COCO annotations\n",
        "with open(annotation_file, \"r\") as f:\n",
        "    coco_data = json.load(f)\n",
        "\n",
        "# Get a random image from the dataset\n",
        "random_image = random.choice(coco_data[\"images\"])\n",
        "image_path = os.path.join(image_dir, random_image[\"file_name\"])\n",
        "\n",
        "# Load image\n",
        "image = cv2.imread(image_path)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Find corresponding annotations\n",
        "image_id = random_image[\"id\"]\n",
        "bboxes = [ann[\"bbox\"] for ann in coco_data[\"annotations\"] if ann[\"image_id\"] == image_id]\n",
        "labels = [ann[\"category_id\"] for ann in coco_data[\"annotations\"] if ann[\"image_id\"] == image_id]\n",
        "\n",
        "# Draw bounding boxes\n",
        "for bbox, label in zip(bboxes, labels):\n",
        "    x, y, w, h = map(int, bbox)\n",
        "    color = [random.randint(0, 255) for _ in range(3)]\n",
        "    cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
        "    label_name = next((cat[\"name\"] for cat in coco_data[\"categories\"] if cat[\"id\"] == label), \"Unknown\")\n",
        "    cv2.putText(image, label_name, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "# Show image with bounding boxes\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"COCO Annotations Visualization\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lm9-50gLGB8X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.transforms import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "\n",
        "# Load Faster R-CNN model\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
        "num_classes = 7  # 6 classes + background\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/tomato_ripeness_classifier/fasterrcnn_tomato_model/model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define the transformation function\n",
        "def transform_fn(image, target):\n",
        "    # Resize the image to a fixed size\n",
        "    image = transforms.Resize((800, 800))(image)  # Choose your desired size\n",
        "\n",
        "    # Convert COCO annotations to the format expected by MeanAveragePrecision\n",
        "    new_target = []\n",
        "    for ann in target:\n",
        "        new_target.append({\n",
        "            'boxes': torch.tensor(ann['bbox'], dtype=torch.float32).unsqueeze(0), # Convert boxes to tensor\n",
        "            'labels': torch.tensor([ann['category_id']], dtype=torch.int64) # Convert category_id to tensor\n",
        "        })\n",
        "    return F.to_tensor(image), new_target\n",
        "\n",
        "# Load validation dataset (COCO format)\n",
        "val_dataset = CocoDetection(\n",
        "    root=\"/content/drive/MyDrive/tomato_ripeness_classifier/dataset_augmented/val/images\",\n",
        "    annFile=\"/content/drive/MyDrive/tomato_ripeness_classifier/dataset_augmented/val/annotations.json\",\n",
        "    transforms=transform_fn\n",
        ")\n",
        "\n",
        "# Define a custom collate function to handle variable-sized images\n",
        "def collate_fn(batch):\n",
        "    images, targets = zip(*batch)  # Unpack images and targets\n",
        "    images = list(images)  # Convert to list for variable sizes\n",
        "    targets = list(targets)  # Convert to list\n",
        "    return images, targets\n",
        "\n",
        "# Use DataLoader with the custom collate function\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Initialize metric\n",
        "metric = MeanAveragePrecision().to(device)\n",
        "\n",
        "# Run evaluation\n",
        "for images, targets in val_loader:\n",
        "    images = [img.to(device) for img in images]\n",
        "    targets = [{k: v.to(device) for k, v in t[0].items()} for t in targets] # Move targets to device\n",
        "    with torch.no_grad():\n",
        "        preds = model(images)\n",
        "    metric.update(preds, targets)\n",
        "\n",
        "# Compute and display mAP results\n",
        "map_metrics = metric.compute()\n",
        "print(map_metrics)\n",
        "\n",
        "# Visualize mAP per class\n",
        "class_names = [\"b_fully_ripened\", \"b_half_ripened\", \"b_green\", \"l_fully_ripened\", \"l_half_ripened\", \"l_green\"]  # Your class names\n",
        "map_per_class = map_metrics[\"map_per_class\"]\n",
        "\n",
        "# Check if map_per_class is available\n",
        "if map_per_class is not None:\n",
        "    map_values = map_per_class.cpu().numpy()  # Move to CPU and convert to NumPy\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(class_names, map_values)\n",
        "    plt.xlabel(\"Classes\")\n",
        "    plt.ylabel(\"mAP\")\n",
        "    plt.title(\"mAP per Class\")\n",
        "    plt.xticks(rotation=45, ha=\"right\")  # Rotate x-axis labels for better readability\n",
        "    plt.tight_layout()  # Adjust layout to prevent labels from being cut off\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"mAP per class is not available in the metrics.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNZDOoKZOjKs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "# Load the trained YOLOv8 model\n",
        "from ultralytics import YOLO\n",
        "yolo_model = YOLO(\"/content/drive/MyDrive/tomato_ripeness_classifier/yolov8_tomato_ripeness/weights/best.pt\")\n",
        "\n",
        "# Load the trained Faster R-CNN model\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "frcnn_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
        "num_classes = 7  # 6 classes + background\n",
        "in_features = frcnn_model.roi_heads.box_predictor.cls_score.in_features\n",
        "frcnn_model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "frcnn_model.load_state_dict(torch.load(\"/content/drive/MyDrive/tomato_ripeness_classifier/fasterrcnn_tomato_model/model.pth\"))\n",
        "frcnn_model.eval()\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "frcnn_model.to(device)\n",
        "\n",
        "# Class names from your dataset\n",
        "class_names = [\"b_fully_ripened\", \"b_half_ripened\", \"b_green\", \"l_fully_ripened\", \"l_half_ripened\", \"l_green\"]\n",
        "\n",
        "# Get a Random Image from the Dataset\n",
        "\n",
        "image_folder = \"/content/dataset/dataset/val/images\"\n",
        "random_image_name = random.choice(os.listdir(image_folder))\n",
        "img_path = os.path.join(image_folder, random_image_name)\n",
        "\n",
        "# Load the image\n",
        "image = Image.open(img_path).convert(\"RGB\")\n",
        "print(f\"Randomly Selected Image: {random_image_name}\")\n",
        "\n",
        "# Function to draw bounding boxes\n",
        "def draw_boxes(image, boxes, labels, scores, threshold=0.5):\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    font = ImageFont.load_default()\n",
        "\n",
        "    for box, label, score in zip(boxes, labels, scores):\n",
        "        if score < threshold:\n",
        "            continue\n",
        "        x1, y1, x2, y2 = box\n",
        "        label_text = f\"{class_names[label]}: {score:.2f}\"\n",
        "\n",
        "        # Draw rectangle\n",
        "        draw.rectangle([(x1, y1), (x2, y2)], outline=\"red\", width=3)\n",
        "\n",
        "        # Draw label\n",
        "        text_size = draw.textbbox((0, 0), label_text, font=font)\n",
        "        draw.rectangle([x1, y1 - text_size[3], x1 + text_size[2], y1], fill=\"red\")\n",
        "        draw.text((x1, y1 - text_size[3]), label_text, fill=\"white\", font=font)\n",
        "\n",
        "    return image\n",
        "\n",
        "# YOLOv8 Inference\n",
        "\n",
        "yolo_preds = yolo_model(img_path, conf=0.5)\n",
        "\n",
        "# Get YOLOv8 detections\n",
        "yolo_classes = [class_names[int(box.cls)] for box in yolo_preds[0].boxes]\n",
        "yolo_counts = dict(Counter(yolo_classes))  # Count occurrences of each class\n",
        "\n",
        "print(\"\\nðŸ“Œ YOLOv8 Detections:\")\n",
        "for cls, count in yolo_counts.items():\n",
        "    print(f\"ðŸ”¹ {cls}: {count}\")\n",
        "\n",
        "# Show YOLO detections\n",
        "yolo_preds[0].show()\n",
        "\n",
        "# Faster R-CNN Inference\n",
        "\n",
        "image_tensor = F.to_tensor(image).unsqueeze(0).to(device)\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    frcnn_preds = frcnn_model(image_tensor)\n",
        "\n",
        "# Process predictions\n",
        "boxes = frcnn_preds[0]['boxes'].cpu().numpy()  # Bounding boxes\n",
        "labels = frcnn_preds[0]['labels'].cpu().numpy() - 1  # Class labels (adjust for 0-based indexing)\n",
        "scores = frcnn_preds[0]['scores'].cpu().numpy()  # Confidence scores\n",
        "\n",
        "# Filter predictions (Threshold = 0.5)\n",
        "valid_indices = scores > 0.5\n",
        "boxes = boxes[valid_indices]\n",
        "labels = labels[valid_indices]\n",
        "scores = scores[valid_indices]\n",
        "\n",
        "# Count class detections for Faster R-CNN\n",
        "frcnn_classes = [class_names[label] for label in labels]\n",
        "frcnn_counts = dict(Counter(frcnn_classes))  # Count occurrences of each class\n",
        "\n",
        "print(\"\\nðŸ“Œ Faster R-CNN Detections:\")\n",
        "for cls, count in frcnn_counts.items():\n",
        "    print(f\"ðŸ”¹ {cls}: {count}\")\n",
        "\n",
        "# Draw Faster R-CNN detections on the image\n",
        "frcnn_image = draw_boxes(image.copy(), boxes, labels, scores)\n",
        "\n",
        "# Show the Faster R-CNN results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(frcnn_image)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Faster R-CNN Predictions\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNwfyO3JKp0UNgxyeggj421",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}